#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏ - –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import joblib

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

MODEL_DIR = Path('models')
DATASET_PATH = Path('dataset.csv')


def main():
    logger.info("=" * 80)
    logger.info("üöÄ –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ò - –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø")
    logger.info("=" * 80)

    if not DATASET_PATH.exists():
        logger.error(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {DATASET_PATH}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞
    logger.info("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    df = pd.read_csv(DATASET_PATH)
    logger.info(f"   –°—Ç—Ä–æ–∫: {len(df)}")
    logger.info(f"   Actionable=1: {(df['actionable'] == 1).sum()}")
    logger.info(f"   Actionable=0: {(df['actionable'] == 0).sum()}")

    X = df['text'].str.lower()
    y = df['actionable'].values

    # Split
    logger.info("\n‚úÇÔ∏è  –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    logger.info(f"   Train: {len(X_train)}, Test: {len(X_test)}")

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø
    logger.info("\nüìù –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (TF-IDF, –º–∞–∫—Å–∏–º—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)...")
    vectorizer = TfidfVectorizer(
        max_features=2000,  # –ë–û–õ–¨–®–ï –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        ngram_range=(1, 3),  # –¢—Ä–∏–≥—Ä–∞–º–º—ã —Ç–æ–∂–µ!
        min_df=1,  # –î–∞–∂–µ –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞
        max_df=0.99,
        lowercase=True,
        token_pattern=r'(?u)\b\w+\b',  # –í—Å–µ —Å–ª–æ–≤–∞
        strip_accents='unicode',
        analyzer='word'
    )

    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    logger.info(f"   Shape: {X_train_vec.shape}")
    logger.info(f"   Features: {len(vectorizer.get_feature_names_out())}")

    # –û–±—É—á–µ–Ω–∏–µ - –ú–ê–ö–°–ò–ú–£–ú –ü–ê–†–ê–ú–ï–¢–†–û–í
    logger.info("\nüß† –û–±—É—á–µ–Ω–∏–µ RandomForest (–º–∞–∫—Å–∏–º—É–º –¥–µ—Ä–µ–≤—å–µ–≤)...")
    model = RandomForestClassifier(
        n_estimators=500,  # –ú–ù–û–ì–û –¥–µ—Ä–µ–≤—å–µ–≤
        max_depth=25,  # –ì–õ–£–ë–ñ–ï
        min_samples_split=2,  # –ú–µ–Ω—å—à–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        class_weight='balanced_subsample',  # –õ–£–ß–®–ï –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
        bootstrap=True,
        oob_score=True
    )

    model.fit(X_train_vec, y_train)

    # –û—Ü–µ–Ω–∫–∞
    logger.info("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    train_score = model.score(X_train_vec, y_train)
    test_score = model.score(X_test_vec, y_test)
    oob_score = model.oob_score_

    logger.info(f"   ‚úÖ Train accuracy: {train_score * 100:.2f}%")
    logger.info(f"   ‚úÖ Test accuracy: {test_score * 100:.2f}%")
    logger.info(f"   ‚úÖ OOB score: {oob_score * 100:.2f}%")

    y_pred = model.predict(X_test_vec)

    logger.info("\n" + classification_report(y_test, y_pred,
                                             target_names=['Non-actionable', 'Actionable']))

    cm = confusion_matrix(y_test, y_pred)
    logger.info(f"\nConfusion Matrix:\n{cm}")

    # –í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    logger.info("\nüìä –¢–æ–ø 30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    feature_names = vectorizer.get_feature_names_out()
    feature_importance = model.feature_importances_

    top_indices = np.argsort(feature_importance)[-30:][::-1]
    for i, idx in enumerate(top_indices, 1):
        importance = feature_importance[idx]
        logger.info(f"   {i:2d}. {feature_names[idx]:30} ‚Üí {importance:.4f}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, MODEL_DIR / 'model.pkl')
    joblib.dump(vectorizer, MODEL_DIR / 'vectorizer.pkl')
    logger.info(f"   ‚úÖ model.pkl —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({MODEL_DIR / 'model.pkl'})")
    logger.info(f"   ‚úÖ vectorizer.pkl —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({MODEL_DIR / 'vectorizer.pkl'})")

    # –¢–µ—Å—Ç –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    logger.info("\nüß™ –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï:")
    test_texts = [
        ("–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ2 - –¥–∞—Ç–∞: 13.02", 1, "–Ω–æ—Ä–º –∑–∞–¥–∞–Ω–∏–µ —Å –¥–∞—Ç–æ–π"),
        ("–¢–∏–ø–æ–≤–æ–π —Ä–∞—Å—á–µ—Ç ‚Ññ1: –î–ú –î–∞—Ç–∞: 24:10", 1, "—Ç–∏–ø–æ–≤–∏–∫ —Å –¥–∞—Ç–æ–π"),
        ("–ö–æ–ª–ª–æ–∫–≤–∏—É–º –∑–∞–≤—Ç—Ä–∞ –≤ 15:30", 1, "–∫–æ–ª–ª–æ–∫–≤–∏—É–º —Å –≤—Ä–µ–º–µ–Ω–µ–º"),
        ("—Å—ã–Ω—à–∞–ª–∞–≤—ã", 0, "–±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"),
        ("—Ö–∞—Ö–∞—Ö–∞—Ö–∞", 0, "—Å–º–µ—Ö"),
        ("–¥—É—Ä–∞", 0, "–æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏–µ"),
        ("–ó–∞–≤—Ç—Ä–∞ –≤ 15:00 –±—É–¥–µ—Ç –∑–∞–ø–∏—Å—å", 0, "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø–∏—Å–∏"),
        ("–û—Ç–∫—Ä–æ–µ—Ç—Å—è –∑–∞–ø–∏—Å—å –Ω–∞ —ç–∫–∑–∞–º–µ–Ω—ã", 0, "–æ—Ç–∫—Ä–æ–µ—Ç—Å—è - –Ω–µ actionable"),
        ("–ø–µ—Ä–µ–Ω–æ—Å –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é", 0, "–ø–µ—Ä–µ–Ω–æ—Å - –Ω–µ actionable"),
        ("–î–æ—Å–¥–∞—á–∞ –ö–æ–ª–ª–æ–∫ ‚Ññ3 –∏ ‚Ññ4 - –æ–ø | –¥–∞—Ç–∞ - 11.3", 1, "–¥–æ—Å–¥–∞—á–∞ —Å –¥–∞—Ç–æ–π"),
        ("–∞—Ö–∞—Ö–∞—Ö–∞", 0, "—Å–º–µ—Ö 2"),
        ("–±–ª—è—Ç—å", 0, "—Ä—É–≥–∞–Ω—å"),
    ]

    correct = 0
    for text, expected, description in test_texts:
        X = vectorizer.transform([text])
        pred = model.predict(X)[0]
        proba = model.predict_proba(X)[0]

        is_correct = pred == expected
        if is_correct:
            correct += 1

        status = "‚úÖ" if is_correct else "‚ùå"
        confidence = proba[1]
        pred_name = "‚úì" if pred == 1 else "‚úó"

        logger.info(f"   {status} [{pred_name}] {text[:40]:40} ‚Üí {confidence * 100:5.1f}% | {description}")

    logger.info(f"\n   üéØ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö: {correct}/{len(test_texts)} ({correct * 100 // len(test_texts)}%)")

    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("=" * 80)


if __name__ == '__main__':
    main()